{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae0f068",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The following cell will import various packages that're needed for this text processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a89801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some general import statements\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import statements related to Gensim + NLTK\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import phrases\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import statements for the visualization tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting up spaCy's nlp module\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Grab the stopwords from NLTK\n",
    "stopWords = stopwords.words(\"english\")\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0923a3d6",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In order to process the tweets, I'll need to pull them from each of the JSONs and put them into a reasonable form. I'm going to create a dict where: \n",
    "\n",
    "*account name* --> {\"tweets\": \\[tweet1, tweet2, ...\\], \"retweets: ..., \"likes\": ... }\n",
    "\n",
    "Each of the tweets (whether they be original Tweets, Likes, or Retweets) will be a string. \n",
    "\n",
    "In order to properly run the rest of the notebook, you ought to split data into two folders: \"Blocked\" and \"Not-Blocked\". Then, insert their paths below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked_folder_path = \"\"\n",
    "not_blocked_folder_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e03834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate through the folder where the JSONs are stored and store their paths\n",
    "jsonPathList = []\n",
    "jsonFolders = []\n",
    "jsonFolders.append(Path(blocked_folder_path))\n",
    "jsonFolders.append(Path(not_blocked_folder_path))\n",
    "for jsonFolder in jsonFolders:\n",
    "    for path in jsonFolder.iterdir():\n",
    "        if (path.suffix==\".json\"):\n",
    "            jsonPathList.append(path)\n",
    "    \n",
    "# Read all of the JSONs \n",
    "totalTweetCt = tweetCt = retweetCt = likeCt = 0\n",
    "accountData = {}\n",
    "for jsonPath in jsonPathList:\n",
    "    with open(jsonPath, \"r\") as jsonFile:\n",
    "        userType = jsonPath.parents[0].stem\n",
    "        jsonData = json.load(jsonFile)\n",
    "        username = jsonData[\"username\"]\n",
    "        textDict = {\"type\": userType,\n",
    "                    \"tweets\": jsonData[\"tweets\"], \n",
    "                    \"retweets\": jsonData[\"retweets\"], \n",
    "                    \"likes\": jsonData[\"likes\"]}\n",
    "        accountData[username] = textDict\n",
    "        totalTweetCt += len(jsonData[\"tweets\"]) + len(jsonData[\"retweets\"]) + len(jsonData[\"likes\"])\n",
    "        tweetCt += len(jsonData[\"tweets\"])\n",
    "        retweetCt += len(jsonData[\"retweets\"])\n",
    "        likeCt += len(jsonData[\"likes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all of the blocked / friendly accounts\n",
    "accountTypeDict = {}\n",
    "for account, dataDict in accountData.items():\n",
    "    if (dataDict[\"type\"] not in accountTypeDict):\n",
    "        accountTypeDict[dataDict[\"type\"]] = []\n",
    "    accountTypeDict[dataDict[\"type\"]].append(account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will extract a single string version of a Tweet\n",
    "def extractTweetString(tweet, isRetweet=False):\n",
    "    fullText = \"\"\n",
    "    if (isRetweet): fullText = tweet[\"retweeted_status\"][\"full_text\"]\n",
    "    else: \n",
    "        if (\"full_text\" in tweet): fullText = tweet[\"full_text\"]\n",
    "        elif (\"text\" in tweet): fullText = tweet[\"text\"]\n",
    "    return fullText\n",
    "\n",
    "# This will save \"extracted Tweet\" versions of each Tweet in the accountTextData dict\n",
    "accountTextData = {}\n",
    "for account in accountData.keys():\n",
    "    tweets = accountData[account][\"tweets\"]\n",
    "    retweets = accountData[account][\"retweets\"]\n",
    "    likes = accountData[account][\"likes\"]\n",
    "    accountTextData[account] = {\"tweets\": [extractTweetString(x) for x in tweets],\n",
    "                                \"retweets\": [extractTweetString(x, True) for x in retweets],\n",
    "                                \"likes\": [extractTweetString(x) for x in likes]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4979599d",
   "metadata": {},
   "source": [
    "## Pre-Processing Text \n",
    "\n",
    "The cell below defines a method that'll be used to pre-process the text contained in a Tweet. The cells afterwards preprocess each of the Tweets in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d7409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will run through various pre-processing steps on Tweets \n",
    "def preprocessTweet(tweet):\n",
    "    \n",
    "    # Add a space at the end of the tweet (helps w/ processing)\n",
    "    tweet = tweet + \" \"\n",
    "    \n",
    "    # Make everything in the tweet lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove any user account tags (i.e., @____) from the Tweet\n",
    "    tweet = re.sub(r'@[a-z]+\\s', \"\", tweet)\n",
    "    \n",
    "    # Remove any links from the Tweet\n",
    "    tweet = re.sub(r'https:\\/\\/.+\\/.+\\s', \"\", tweet)\n",
    "    \n",
    "    # Do some \"simple pre-processing\" to tokenize / remove punctuation\n",
    "    tweet = simple_preprocess(tweet)\n",
    "    \n",
    "    # Remove any of the English stopwords from the Tweet\n",
    "    tweet = [x for x in tweet if x not in stopWords]\n",
    "    \n",
    "    # Return the preprocessed Tweet\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e61d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict w/ each accounts' Tweets processed\n",
    "processedAccountTextData = {}\n",
    "for account, textDict in accountTextData.items():\n",
    "    newTextDict = {k:[] for k in textDict.keys()}\n",
    "    for key, tweetList in textDict.items():\n",
    "        for tweet in tweetList:\n",
    "            newTextDict[key].append(preprocessTweet(tweet))\n",
    "    processedAccountTextData[account] = newTextDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58badff5",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "There are a couple of things I'm interested in figuring out about the textual data before doing any NLP on it! Some of those things include: \n",
    "\n",
    "- What's the distribution of \"Tweets\" vs. \"Retweets\" for each of the accounts?\n",
    "- What's the average Tweet length? \n",
    "- What does a word cloud look like for any particular user? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba07af",
   "metadata": {},
   "source": [
    "### Tweets vs. Retweets \n",
    "\n",
    "The cell below will create a scatterplot, where the x-axis is \"Number of Tweets\" and the y-axis is \"Number of Retweets\". Since I can't actually pull the Tweets and Retweets separately, I just pulled \"the most recent 400 Tweets\" - it might be interesting to understand how everything is distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f83d60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Count each users tweet / retweet count, and store this in arrays\n",
    "tweetCt = []\n",
    "retweetCt = []\n",
    "for account in accountData.keys():\n",
    "    tweetCt.append(len(accountData[account][\"tweets\"]))\n",
    "    retweetCt.append(len(accountData[account][\"retweets\"]))\n",
    "\n",
    "# Create a scatterplot of \"Tweet count\" vs. \"Retweet count\"\n",
    "plt.scatter(tweetCt, retweetCt)\n",
    "plt.title(\"Tweet vs. Retweets (450 most recent Tweets)\")\n",
    "plt.xlabel(\"Tweets\")\n",
    "plt.ylabel(\"Retweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a268f0d",
   "metadata": {},
   "source": [
    "### Average Tweet Length\n",
    "\n",
    "The cell below will try to calculate the \"average Tweet length\" for all of the users' Tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fced027",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This method will return the length of a given Tweet\n",
    "def tweetLength(tweet): \n",
    "    splitTweet = tweet.split()\n",
    "    return len(splitTweet)\n",
    "    \n",
    "# Iterate through each account and calculate an \"average Tweet length\"\n",
    "for account in accountTextData.keys():\n",
    "    tweetLengths = []\n",
    "    for tweet in accountTextData[account][\"tweets\"]:\n",
    "        tweetLengths.append(tweetLength(tweet))\n",
    "    avgLength = sum(tweetLengths)/len(tweetLengths)\n",
    "    print(\"%s: %.3f\" % (account, avgLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4793b098",
   "metadata": {},
   "source": [
    "### Word Clouds\n",
    "\n",
    "The cells below will attempt to create word clouds from individual users' Tweets / Retweets / Likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9853a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will compile the tweets for a given list of users into a \n",
    "# single text string; this will be used as input for the word cloud\n",
    "def collectText(accountList):\n",
    "    tokens = []\n",
    "    for account in accountList:\n",
    "        textDict = processedAccountTextData[account]\n",
    "        for tweetType, tweetList in textDict.items():\n",
    "            for tweet in tweetList:\n",
    "                tokens = tokens + tweet\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# When given two strings from the \"collectText\" method, this method will\n",
    "# attempt to generate a list of the words that appear more frequently\n",
    "# in either string. \n",
    "def compareCollectedText(str1, str2, compSize=100):\n",
    "    \n",
    "    # First, create corpuses of both strings \n",
    "    corpusList = []\n",
    "    id2wordList = []\n",
    "    for curStr in [str1, str2]:\n",
    "        id2word = corpora.Dictionary([curStr.split()])\n",
    "        corpus = id2word.doc2bow(curStr.lower().split())\n",
    "        corpus = sorted(corpus, key = lambda x: x[1], reverse=True)\n",
    "        corpusList.append(corpus)\n",
    "        id2wordList.append(id2word)\n",
    "        \n",
    "    # Next, iterate through the top 100 in each to see which words are unique\n",
    "    corpus1_top = [x[0] for x in corpusList[0][:compSize]]\n",
    "    corpus2_top = [x[0] for x in corpusList[1][:compSize]]\n",
    "    corpus1_unique = []\n",
    "    corpus2_unique = []\n",
    "    \n",
    "    corpus1_wordList = set([id2wordList[0][x] for x in corpus1_top])\n",
    "    corpus2_wordList = set([id2wordList[1][x] for x in corpus2_top])\n",
    "    \n",
    "    print(corpus1_wordList)\n",
    "    print(corpus2_wordList)\n",
    "    \n",
    "    # Dealing w/ corpus1\n",
    "    for wordID in corpus1_top:\n",
    "        corpus1Word = id2wordList[0][wordID]\n",
    "        if (corpus1Word not in corpus2_wordList): \n",
    "            corpus1_unique.append(wordID)\n",
    "    print(corpus1_unique)\n",
    "    print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "    \n",
    "    # Dealing with corpus2\n",
    "    for wordID in corpus2_top:\n",
    "        corpus2Word = id2wordList[1][wordID]\n",
    "        if (corpus2Word not in corpus1_wordList): \n",
    "            corpus2_unique.append(wordID)\n",
    "    print(corpus2_unique)\n",
    "            \n",
    "    # Print the lengths of the unique word lists\n",
    "    print(\"corpus 1 has %d unqiue words\" % len(corpus1_unique))\n",
    "    print(\"corpus 2 has %d unqiue words\" % len(corpus2_unique))\n",
    "    \n",
    "    # Creating dictionaries from the corpuses\n",
    "    corpusDictList = []\n",
    "    for corpus in corpusList:\n",
    "        corpusDict = {}\n",
    "        for wordID, freq in corpus:\n",
    "            corpusDict[wordID] = freq\n",
    "        corpusDictList.append(corpusDict)\n",
    "    \n",
    "    # Create strings representing the frequency of words within the top 100\n",
    "    newStr = []\n",
    "    for corpusIdx, corpus in enumerate([corpus1_unique, corpus2_unique]):\n",
    "        words = []\n",
    "        uniqueWords = []\n",
    "        for wordID in corpus:\n",
    "            word = (id2wordList[corpusIdx][wordID])\n",
    "            uniqueWords.append(word)\n",
    "            wordFreq = (corpusDictList[corpusIdx][wordID])\n",
    "            repeatedWord = (word + \" \") * wordFreq\n",
    "            words = words + repeatedWord.split()\n",
    "        random.shuffle(words)\n",
    "        corpusStr = \" \".join(words)\n",
    "        newStr.append(corpusStr)\n",
    "    \n",
    "    # Return the new strings that're representative of the comparison \n",
    "    # between these two corpus\n",
    "    return newStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e26bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "friendStr = collectText(accountTypeDict[\"Not-Blocked\"])\n",
    "blockStr = collectText(accountTypeDict[\"Blocked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = friendStr\n",
    "friendStr = blockStr\n",
    "blockStr = temp\n",
    "print(len(blockStr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(friendStr))\n",
    "print(len(blockStr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5453bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "newStrs = compareCollectedText(friendStr, blockStr, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07095bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed for visualizing the word cloud\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create and show the wordcloud\n",
    "friendUnique, blockUnique = newStrs\n",
    "scale = 8\n",
    "cloud = WordCloud(width=400*scale, height=200*scale).generate(blockUnique)\n",
    "cloud.to_file(\"../Visualizations/Blocked Word Cloud UNIQUE.png\")\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be6399",
   "metadata": {},
   "source": [
    "### LDA Topic Modelling\n",
    "\n",
    "This is technically entering the \"NLP\" phase, but it's still a part of data exploration. I want to try and understand the different topics being represented in this data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81431e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "accountTextList = []\n",
    "for account in processedAccountTextData.keys():\n",
    "    accountTextList.append(collectText([account]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349dd2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will iterate through accountTextList and create some \n",
    "# necessary data structures for the LDA\n",
    "bigram = phrases.Phraser(phrases.Phrases(accountTextList, min_count=3, threshold=100))\n",
    "trigram = phrases.Phraser(phrases.Phrases(bigram[accountTextList], threshold=100))\n",
    "processedAccountTextList = [trigram[x] for x in accountTextList]\n",
    "id2word = corpora.Dictionary(processedAccountTextList)\n",
    "corpus = [id2word.doc2bow(account) for account in processedAccountTextList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create the LDA model using the data structures above\n",
    "model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word,\n",
    "                                        num_topics=6, random_state=255,\n",
    "                                        update_every=1, chunksize=100,\n",
    "                                        passes=20, alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d25ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This cell will visualize the topics in the topic model\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ed888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will look through *numWords* most associated w/ a topic, and\n",
    "# return a list of all of the most popular words\n",
    "def popularWords(model, numWords):\n",
    "    topicKeywords = model.show_topics(num_words=numWords, formatted=False)\n",
    "    topics = []\n",
    "    for topicNum, wordList in topicKeywords:\n",
    "        phrases = []\n",
    "        for word, prob in wordList:\n",
    "            phrases.append((word, prob))\n",
    "        topics.append(phrases)\n",
    "    return topics\n",
    "\n",
    "# This method will look through *numWords* most associated w/ a topic, \n",
    "# and return a list of all of the multi-word phrases from that topic\n",
    "def popularPhrases(model, numWords):\n",
    "    topicKeywords = model.show_topics(num_words=numWords, formatted=False)\n",
    "    topics = []\n",
    "    for topicNum, wordList in topicKeywords:\n",
    "        phrases = []\n",
    "        for word, prob in wordList:\n",
    "            if (\"_\" in word): phrases.append((word, prob))\n",
    "        topics.append(phrases)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03c90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for wordIdx, wordGroup in enumerate(popularWords(model, 40)):\n",
    "    print(\"\\n\\nGROUP %d:\\n%s\\n\\n\" % (wordIdx, wordGroup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d9138",
   "metadata": {},
   "outputs": [],
   "source": [
    "for wordIdx, wordGroup in enumerate(popularPhrases(model, 400)):\n",
    "    print(\"\\n\\nGROUP %d:\\n%s\\n\\n\" % (wordIdx, wordGroup))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2de6c1",
   "metadata": {},
   "source": [
    "## Word2Vec Embeddings\n",
    "\n",
    "We're going to use Gensim's word2vec models to learn word embeddings for all of the words in the accounts! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333cd026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(processedAccountTextList, min_count=1, vector_size=128, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a432691",
   "metadata": {},
   "source": [
    "### Visualizing Embeddings\n",
    "\n",
    "Here, we want to visualize the embeddings on an account level. We're going to calculate embeddings for each account by averaging together all of the word embeddings used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e10b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedAccountTextDict = {}\n",
    "for idx, account in enumerate(processedAccountTextData.keys()):\n",
    "    processedAccountTextDict[account] = processedAccountTextList[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will calculate the \"average word embedding\" for an account\n",
    "def getAccountWordVec(model, account):\n",
    "    tokenList = (processedAccountTextDict[account])\n",
    "    wvList = np.array([model.wv[token] for token in tokenList])\n",
    "    return np.mean(wvList, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we'll calculate average word embeddings for each account\n",
    "avgEmbeddingDict = {}\n",
    "for acctType in accountTypeDict.keys():\n",
    "    avgEmbeddingDict[acctType] = []\n",
    "    for account in accountTypeDict[acctType]:\n",
    "        avgEmbeddingDict[acctType].append((account, getAccountWordVec(model, account)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34720878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for each of the accounts\n",
    "blockedAcct_x = [x[1][0] for x in avgEmbeddingDict[\"Blocked\"]]\n",
    "blockedAcct_y = [x[1][1] for x in avgEmbeddingDict[\"Blocked\"]]\n",
    "friendAcct_x = [x[1][0] for x in avgEmbeddingDict[\"Not-Blocked\"]]\n",
    "friendAcct_y = [x[1][1] for x in avgEmbeddingDict[\"Not-Blocked\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c19a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot these\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 2, 2])\n",
    "blocked = ax.scatter(blockedAcct_x, blockedAcct_y, color=\"red\", alpha=.3)\n",
    "friends = ax.scatter(friendAcct_x, friendAcct_y, color=\"green\", alpha=.3)\n",
    "plt.title(\"2-Dimensional Word Embedding of Twitter Network\")\n",
    "plt.legend((blocked, friends), (\"Blocked\", \"Non-Blocked\"))\n",
    "plt.savefig(\"../Visualizations/WordEmb2.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
