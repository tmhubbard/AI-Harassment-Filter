{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a040f29",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "The following cells are involved in all sorts of setup. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49dbf08",
   "metadata": {},
   "source": [
    "### Import Statements\n",
    "\n",
    "The cell below imports a number of libraries / functions that're essential for the remainder of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c11f971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some import statements\n",
    "import torch, json, random, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "\n",
    "# Setting up Pytorch's use of CUDA; if your computer isn't CUDA-enabled, \n",
    "# you could replace \"cuda\" with \"cpu\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed23f8",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "The following cell will load both kinds of account embeddings (network & text) into a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63eaa842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This class will be used to load in the account data\n",
    "class AccountDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    # The __init__ method defines how we'll input the data\n",
    "    def __init__(self, networkEmbPath, textEmbPath):  \n",
    "        \n",
    "        # First, open the network embedding file and extract the embeddings\n",
    "        networkEmbDict = {}\n",
    "        with open(networkEmbPath, \"r\") as networkEmbFile:\n",
    "            networkEmbDict = json.load(networkEmbFile)\n",
    "                \n",
    "        # Next, open the text embedding file, and extract the embeddings\n",
    "        textEmbDict = {}\n",
    "        with open(textEmbPath, \"r\") as textEmbFile:\n",
    "            textEmbDict = json.load(textEmbFile)\n",
    "                    \n",
    "        # Open the \"Account Labels.json\" file to indicate which accounts\n",
    "        # are Friends / Blocked\n",
    "        account_type_mapping = {\"blocked\": \"Blocked\", \"not-blocked\": \"Friends\"}\n",
    "        acctTypeDict = {}\n",
    "        with open(\"../Data/Account Labels.json\", \"r\") as acctTypeJSON:\n",
    "            account_types = json.load(acctTypeJSON)\n",
    "            for account_type, account_list in account_types.items():\n",
    "                for account in account_list:\n",
    "                    acctTypeDict[str(account)] = account_type_mapping[account_type]\n",
    "                \n",
    "        # Join together the embeddings of the Friends / Blocked accounts\n",
    "        # that have both a text embedding and a social network embedding\n",
    "        accountDict = {}\n",
    "        blockCt = 0\n",
    "        friendCt = 0\n",
    "        for acct, acctType in acctTypeDict.items():\n",
    "            inBoth = (acct in networkEmbDict and acct in textEmbDict)\n",
    "            if (acctType != \"Adjacent\" and inBoth):\n",
    "                networkEmb = torch.tensor(networkEmbDict[acct], dtype=torch.float32, device=device)\n",
    "                textEmb = torch.tensor(textEmbDict[acct], dtype=torch.float32, device=device)\n",
    "                blocked = 0\n",
    "                if (acctType == \"Blocked\"):\n",
    "                    blocked = 1\n",
    "                    blockCt += 1\n",
    "                else:\n",
    "                    friendCt += 1\n",
    "                blocked = torch.tensor(blocked, dtype=torch.float32, device=device)\n",
    "                accountDict[acct] = {\"networkEmb\": networkEmb, \"textEmb\": textEmb, \"blocked\": blocked}\n",
    "        \n",
    "        # Now, create a list version of the accountDict, and shuffle it\n",
    "        shuffledAccountDict = list(accountDict.keys())\n",
    "        np.random.shuffle(shuffledAccountDict)\n",
    "        self.accountList = []\n",
    "        for userID in shuffledAccountDict:\n",
    "            account = accountDict[userID]\n",
    "            account[\"userID\"] = userID\n",
    "            self.accountList.append(account)\n",
    "            \n",
    "        # Equalize the number of examples in the dataset\n",
    "        # print(\"There are %d friends and %d blocked\" % (friendCt, blockCt))\n",
    "        typeToRemove = 0\n",
    "        amtToRemove = friendCt - blockCt\n",
    "        if (blockCt > friendCt): \n",
    "            amtToRemove = blockCt - friendCt\n",
    "            typeToRemove = 1\n",
    "        idxToRemove = []\n",
    "        startPoint = 0\n",
    "        for idx, account in enumerate(self.accountList):\n",
    "            if (amtToRemove == 0): break\n",
    "            if (account[\"blocked\"] == typeToRemove): \n",
    "                idxToRemove.append(idx)\n",
    "                amtToRemove -= 1\n",
    "        for idx in reversed(idxToRemove):\n",
    "            del self.accountList[idx]\n",
    "            \n",
    "        # Check the amount of examples in the dataset again\n",
    "        newBlockCt = 0\n",
    "        newFriendCt = 0\n",
    "        for acct in self.accountList:\n",
    "            if (acct[\"blocked\"] == 0): newFriendCt += 1\n",
    "            elif (acct[\"blocked\"] == 1): newBlockCt += 1\n",
    "        # print(\"There are %d friends and %d blocked\" % (newFriendCt, newBlockCt))\n",
    "            \n",
    "        \n",
    "    # The \"len\" method will return the number of data points in this Dataset\n",
    "    def __len__(self):\n",
    "        return len(self.accountList)\n",
    "        \n",
    "    # The \"getitem\" method will specify how to return the item\n",
    "    # at a particular index \n",
    "    def __getitem__(self, idx):\n",
    "        if (torch.is_tensor(idx)):\n",
    "            idx = idx.tolist()\n",
    "        account = self.accountList[idx]\n",
    "        userID = account[\"userID\"]\n",
    "        networkEmb = account[\"networkEmb\"]\n",
    "        textEmb = account[\"textEmb\"]\n",
    "        blocked = account[\"blocked\"]\n",
    "        return (userID, networkEmb, textEmb, blocked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a197f92c",
   "metadata": {},
   "source": [
    "### Training / Test Splits\n",
    "\n",
    "Since we've defined the AccountDataset class, we're now able to create a new AccountDataset object, and then create some DataLoaders for training / test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b91c3b37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These cells will actually load the data into the AccountDataset\n",
    "networkEmbeddingPath = \"../Data/Embeddings/node2vec embeddings.json\"\n",
    "textEmbeddingPath = \"../Data/Embeddings/word2vec embeddings.json\"\n",
    "accounts = AccountDataset(networkEmbeddingPath, textEmbeddingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c3ccec5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now, this will split up the accounts into \"training\" and \"test\" sets\n",
    "testSplit = .3\n",
    "accountAmt = len(accounts)\n",
    "splitIdx = int(np.floor(accountAmt * testSplit))\n",
    "indices = list(range(accountAmt))\n",
    "np.random.shuffle(indices)\n",
    "trainIndices = indices[splitIdx:]\n",
    "testIndices = indices[:splitIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "884a3662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders for both the training and test sets\n",
    "batchSize = 8\n",
    "sampler_train = SubsetRandomSampler(trainIndices)\n",
    "sampler_test = SubsetRandomSampler(testIndices)\n",
    "loader_train = torch.utils.data.DataLoader(accounts, batch_size=batchSize, sampler=sampler_train)\n",
    "loader_test = torch.utils.data.DataLoader(accounts, batch_size=batchSize, sampler=sampler_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ea4eb",
   "metadata": {},
   "source": [
    "### Defining CheckAccuracy( ) \n",
    "\n",
    "The following method, CheckAccuracy(), will be used to evaluate the accuracy of a given model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97e0c742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This method will check the accuracy of the model using data from the loader\n",
    "def CheckAccuracy(loader, model, modelType, confusion=False, returnAll=False):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    if (modelType != \"split\"):\n",
    "        model.eval()\n",
    "    yTrue = []\n",
    "    yPred = []\n",
    "    with torch.no_grad():\n",
    "        for (userID, networkEmb, textEmb, blocked) in loader:\n",
    "            binaryScores = []\n",
    "            if (modelType == \"network\"):\n",
    "                binaryScores = torch.round(torch.sigmoid(model(networkEmb))).reshape(blocked.shape)\n",
    "            if (modelType == \"text\"):\n",
    "                binaryScores = torch.round(torch.sigmoid(model(textEmb))).reshape(blocked.shape)\n",
    "            if (modelType == \"split\"):\n",
    "                networkEmb_Model_cutoff, textEmb_Model_cutoff, splitModel = model\n",
    "                networkEmb_Model_cutoff.eval()\n",
    "                textEmb_Model_cutoff.eval()\n",
    "                splitModel.eval()\n",
    "                networkScores = networkEmb_Model_cutoff(networkEmb)\n",
    "                textScores = textEmb_Model_cutoff(textEmb)\n",
    "                inputFeatures = torch.cat((networkScores, textScores), dim=1)\n",
    "                binaryScores = torch.round(torch.sigmoid(splitModel(inputFeatures))).reshape(blocked.shape)\n",
    "            num_correct += (blocked == binaryScores).sum().float()\n",
    "            num_samples += len(blocked)\n",
    "            blockedVector = list(blocked.cpu())\n",
    "            scoresVector = list(binaryScores.cpu())\n",
    "            yTrue = yTrue + blockedVector\n",
    "            yPred = yPred + scoresVector\n",
    "        if (confusion):\n",
    "            f1Score = f1_score(yTrue, yPred)\n",
    "            tn, fp, fn, tp = (confusion_matrix(yTrue, yPred).ravel())\n",
    "\n",
    "        if (not returnAll):\n",
    "            return (num_correct/num_samples)\n",
    "        return ((num_correct/num_samples), f1Score, (tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c850845",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Below, you'll find a variety of cells meant to define the models and train them. There are three main models that're defined: \n",
    "\n",
    "- Social network model\n",
    "- Textual model\n",
    "- Mixed model\n",
    "\n",
    "By understanding the performance of each of the models, we'll be able to better understand which data type (social network embeddings vs. textual data) contains more useful predictive information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51809e2",
   "metadata": {},
   "source": [
    "### Social Network Model\n",
    "\n",
    "This model is meant to make predictions based on the social network embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c01895fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will return the best network model it found from the given \n",
    "# trainData and testData\n",
    "def getNetworkModel(trainData, testData, epochAmt=150): \n",
    "\n",
    "    # Declare the drouput strength \n",
    "    dropoutStrength = 0.4\n",
    "\n",
    "    # Declare a couple of layer sizes\n",
    "    networkEmbDim = len(accounts[0][1])\n",
    "    networkEmb_hidden1 = 512\n",
    "    networkEmb_hidden2 = 256\n",
    "    networkEmb_hidden3 = 128\n",
    "    networkEmb_hidden4 = 64\n",
    "\n",
    "    # Declare the model\n",
    "    networkEmb_model = nn.Sequential(nn.Linear(networkEmbDim, networkEmb_hidden1),\n",
    "                          nn.LeakyReLU(),\n",
    "                          #nn.BatchNorm1d(networkEmb_hidden1),\n",
    "                          nn.Linear(networkEmb_hidden1, networkEmb_hidden2),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.Dropout(dropoutStrength),\n",
    "                          nn.Linear(networkEmb_hidden2, networkEmb_hidden3),\n",
    "                          nn.LeakyReLU(),\n",
    "                          #nn.BatchNorm1d(networkEmb_hidden3),\n",
    "                          nn.Linear(networkEmb_hidden3, networkEmb_hidden4),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.Linear(networkEmb_hidden4, 1))\n",
    "\n",
    "    # Define a number of parameters for training\n",
    "    networkEmb_epochs = epochAmt\n",
    "    networkEmb_LR = 0.00004\n",
    "    networkEmb_model = networkEmb_model.to(device)\n",
    "    networkEmb_opt = optim.Adam(networkEmb_model.parameters(), lr=networkEmb_LR)\n",
    "    networkEmb_testAccList = []\n",
    "    networkEmb_trainAccList = []\n",
    "    bestModel = None\n",
    "    bestTestAcc = 0\n",
    "\n",
    "    # Iterate through each training epoch\n",
    "    for e in range(networkEmb_epochs):\n",
    "        for idx, (userID, networkEmb, textEmb, blocked) in enumerate(trainData):\n",
    "\n",
    "            # Indicate that we're in training mode\n",
    "            networkEmb_model.train() \n",
    "\n",
    "            # Declaring a loss function\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "            # Performing a training step for the network embedding model\n",
    "            scores = networkEmb_model(networkEmb)\n",
    "            scores = scores.reshape(blocked.shape)\n",
    "            loss = loss_fn(scores, blocked)\n",
    "            networkEmb_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            networkEmb_opt.step()\n",
    "\n",
    "        # Print the accuracy of the network embedding model\n",
    "        testAcc = CheckAccuracy(testData, networkEmb_model, \"network\")\n",
    "        trainAcc = CheckAccuracy(trainData, networkEmb_model, \"network\")\n",
    "\n",
    "        if (testAcc > bestTestAcc):\n",
    "            bestModel = copy.deepcopy(networkEmb_model)\n",
    "            bestTestAcc = testAcc\n",
    "            \n",
    "    return bestModel, bestTestAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c1efe7",
   "metadata": {},
   "source": [
    "### Text Embedding Model\n",
    "\n",
    "This model is meant to make predictions based on the account text embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3015a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will return the best network model it found from the given \n",
    "# trainData and testData\n",
    "def getTextModel(trainData, testData, epochAmt=150): \n",
    "\n",
    "    # Declare the drouput strength\n",
    "    dropoutStrength = 0.5\n",
    "\n",
    "    # Declare a couple of layer sizes\n",
    "    textEmbDim = len(accounts[0][2])\n",
    "    textEmb_hidden1 = 2048\n",
    "    textEmb_hidden2 = 1024\n",
    "    textEmb_hidden3 = 512\n",
    "    textEmb_hidden4 = 256\n",
    "\n",
    "    # Declare the model\n",
    "    textEmb_model = nn.Sequential(nn.Linear(textEmbDim, textEmb_hidden1),\n",
    "                          nn.LeakyReLU(),\n",
    "                          #nn.BatchNorm1d(textEmb_hidden1),\n",
    "                          nn.Linear(textEmb_hidden1, textEmb_hidden2),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.Dropout(dropoutStrength),\n",
    "                          #nn.BatchNorm1d(textEmb_hidden2),\n",
    "                          nn.Linear(textEmb_hidden2, textEmb_hidden3),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.Dropout(dropoutStrength),\n",
    "                          #nn.BatchNorm1d(textEmb_hidden3),\n",
    "                          nn.Linear(textEmb_hidden3, textEmb_hidden4),\n",
    "                          nn.LeakyReLU(),\n",
    "                          #nn.BatchNorm1d(textEmb_hidden4),\n",
    "                          nn.Linear(textEmb_hidden4, 1))\n",
    "\n",
    "    # Define a number of parameters for training\n",
    "    textEmb_epochs = epochAmt\n",
    "    textEmb_LR = 0.0001\n",
    "    textEmb_model = textEmb_model.to(device)\n",
    "    textEmb_opt = optim.Adam(textEmb_model.parameters(), lr=textEmb_LR)\n",
    "    textEmb_testAccList = []\n",
    "    textEmb_trainAccList = []\n",
    "\n",
    "    bestModel = None\n",
    "    bestTestAcc = 0\n",
    "\n",
    "    # Iterate through each training epoch\n",
    "    for e in range(textEmb_epochs):\n",
    "        for idx, (userID, textEmb, textEmb, blocked) in enumerate(trainData):\n",
    "\n",
    "            # Indicate that we're in training mode\n",
    "            textEmb_model.train() \n",
    "\n",
    "            # Declaring a loss function\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "            # Performing a training step for the text embedding model\n",
    "            scores = textEmb_model(textEmb)\n",
    "            scores = scores.reshape(blocked.shape)\n",
    "            loss = loss_fn(scores, blocked)\n",
    "            textEmb_opt.zero_grad()\n",
    "            loss.backward()\n",
    "            textEmb_opt.step()\n",
    "\n",
    "        # Print the accuracy of the text embedding model\n",
    "        testAcc = CheckAccuracy(testData, textEmb_model, \"text\")\n",
    "        trainAcc = CheckAccuracy(trainData, textEmb_model, \"text\")\n",
    "\n",
    "        if (testAcc > bestTestAcc):\n",
    "            bestModel = copy.deepcopy(textEmb_model)\n",
    "            bestTestAcc = testAcc\n",
    "            \n",
    "    return bestModel, bestTestAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fad4178",
   "metadata": {},
   "source": [
    "### Mixed Model #1: Bagging\n",
    "\n",
    "The following cell will make predictions based on *both* data types! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c3485cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'networkEmb_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2b0a47e79c1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0myTrue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0myPred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mnetworkEmb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtextEmb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mnum_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'networkEmb_model' is not defined"
     ]
    }
   ],
   "source": [
    "# This bagging \"meta-model\" doesn't require any training, since we're just\n",
    "# averaging the output of two pre-trained models\n",
    "# Store the songIDs for each of the correct songs in bothRightGuesses.\n",
    "bothRightGuesses = []\n",
    "yTrue = []\n",
    "yPred = []\n",
    "networkEmb_model.eval()\n",
    "textEmb_model.eval()\n",
    "num_correct = 0\n",
    "num_samples = 0\n",
    "with torch.no_grad():\n",
    "    for (userID, networkEmb, textEmb, blocked) in loader_test:\n",
    "        networkEmb_Scores = (torch.sigmoid(networkEmb_model(networkEmb))).reshape(blocked.shape)\n",
    "        textEmb_Scores = (torch.sigmoid(textEmb_model(textEmb))).reshape(blocked.shape)\n",
    "        bothScores = (networkEmb_Scores + textEmb_Scores)/2\n",
    "        binaryScores = torch.round(bothScores)\n",
    "        mask = (binaryScores == blocked)\n",
    "        bothRightGuesses += ([int(x) for x in (userID[mask])])\n",
    "        yTrue = yTrue + list(blocked.cpu())\n",
    "        yPred = yPred + list(binaryScores.cpu())\n",
    "\n",
    "# Print the confusion matrix + F1 score for the bagging model\n",
    "f1Score = f1_score(yTrue, yPred)\n",
    "tn, fp, fn, tp = (confusion_matrix(yTrue, yPred).ravel())\n",
    "print(\"True negative: %d\" % tn)\n",
    "print(\"False positive: %d\" % fp)\n",
    "print(\"False negative: %d\" % fn)\n",
    "print(\"True positive: %d\" % tp)\n",
    "print(\"F1 score: %.5f\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which accounts the networkEmb model guesses right on \n",
    "networkEmb_RightGuesses = []\n",
    "networkEmb_model.eval()\n",
    "with torch.no_grad():\n",
    "    for (userID, networkEmb, textEmb, blocked) in loader_test:\n",
    "        binaryScores = torch.round(torch.sigmoid(networkEmb_model(networkEmb))).reshape(blocked.shape)\n",
    "        mask = (binaryScores == blocked)\n",
    "        networkEmb_RightGuesses += ([int(x) for x in (userID[mask])])\n",
    "        \n",
    "# Figure out which accounts the textEmb model guesses right on \n",
    "textEmb_RightGuesses = []\n",
    "textEmb_model.eval()\n",
    "with torch.no_grad():\n",
    "    for (userID, networkEmb, textEmb, blocked) in loader_test:\n",
    "        binaryScores = torch.round(torch.sigmoid(textEmb_model(textEmb))).reshape(blocked.shape)\n",
    "        mask = (binaryScores == blocked)\n",
    "        textEmb_RightGuesses += ([int(x) for x in (userID[mask])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "networkEmbRightGuessesSet = set(networkEmb_RightGuesses)\n",
    "textEmbRightGuessesSet = set(textEmb_RightGuesses)\n",
    "bothRightGuessesSet = set(bothRightGuesses)\n",
    "print(\"There were %d accounts in the test set...\" % len(testIndices))\n",
    "print(\"The networkEmb model got %d accounts correct (%.2f%% accuracy)\" % (len(networkEmbRightGuessesSet), 100 * (len(networkEmbRightGuessesSet)/len(testIndices))))\n",
    "print(\"The textEmb model got %d accounts correct (%.2f%% accuracy)\" % (len(textEmbRightGuessesSet), 100 * (len(textEmbRightGuessesSet)/len(testIndices))))\n",
    "print(\"The bagged model got %d accounts correct (%.2f%% accuracy)\" % (len(bothRightGuessesSet), 100 * (len(bothRightGuessesSet)/len(testIndices))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ceeea",
   "metadata": {},
   "source": [
    "### Mixed Model #2: Split\n",
    "\n",
    "The following cell will make predictions based on *both* data types! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16771548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will return the splitModel\n",
    "def getSplitModel(trainData, testData, epochAmt=150):\n",
    "\n",
    "    dropoutStrength = 0.5\n",
    "\n",
    "    networkEmb_DimCount = len(accounts[0][1])\n",
    "    networkEmb_Hidden1 = 512\n",
    "    networkEmb_Hidden2 = 256\n",
    "    networkEmb_Hidden3 = 128\n",
    "    networkEmb_CutoffAmt = 64\n",
    "    networkEmb_Model_cutoff = nn.Sequential(nn.Linear(networkEmb_DimCount, networkEmb_Hidden1),\n",
    "                          nn.LeakyReLU(),\n",
    "                          #nn.BatchNorm1d(networkEmb_Hidden1),\n",
    "                          nn.Linear(networkEmb_Hidden1, networkEmb_Hidden2),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.Linear(networkEmb_Hidden2, networkEmb_Hidden3),\n",
    "                          nn.LeakyReLU(),\n",
    "                          #nn.BatchNorm1d(networkEmb_Hidden3),\n",
    "                          nn.Linear(networkEmb_Hidden3, networkEmb_CutoffAmt),\n",
    "                          nn.Dropout(dropoutStrength))\n",
    "\n",
    "\n",
    "    textEmb_DimCount = len(accounts[0][2])\n",
    "    textEmb_Hidden1 = 2048 \n",
    "    textEmb_Hidden2 = 1024\n",
    "    textEmb_Hidden3 = 512\n",
    "    textEmb_CutoffAmt = 64 \n",
    "    textEmb_Model_cutoff = nn.Sequential(nn.BatchNorm1d(textEmb_DimCount),\n",
    "                               nn.Linear(textEmb_DimCount, textEmb_Hidden1),\n",
    "                               nn.LeakyReLU(),\n",
    "                               nn.Linear(textEmb_Hidden1, textEmb_Hidden2),\n",
    "                               nn.LeakyReLU(),\n",
    "                               nn.Linear(textEmb_Hidden2, textEmb_Hidden3),\n",
    "                               nn.LeakyReLU(),\n",
    "                               #nn.BatchNorm1d(textEmb_Hidden3),\n",
    "                               nn.Linear(textEmb_Hidden3, textEmb_CutoffAmt),\n",
    "                               nn.Dropout(dropoutStrength))\n",
    "\n",
    "\n",
    "    splitInput = textEmb_CutoffAmt+networkEmb_CutoffAmt\n",
    "    splitHidden1 = 1024\n",
    "    splitHidden2 = 512 \n",
    "    splitHidden3 = 256\n",
    "    splitHidden4 = 128\n",
    "    splitModel = nn.Sequential(nn.Linear(splitInput, splitHidden1),\n",
    "                               nn.LeakyReLU(),\n",
    "                               nn.Dropout(dropoutStrength),\n",
    "                               nn.Linear(splitHidden1, splitHidden2),\n",
    "                               nn.LeakyReLU(),\n",
    "                               nn.Linear(splitHidden2, splitHidden3),\n",
    "                               nn.LeakyReLU(),\n",
    "                               # nn.Dropout(dropoutStrength),\n",
    "                               nn.Linear(splitHidden3, splitHidden4),\n",
    "                               nn.LeakyReLU(),\n",
    "                               nn.Linear(splitHidden4, 1))\n",
    "\n",
    "\n",
    "    splitModel = splitModel.to(device)\n",
    "    textEmb_Model_cutoff = textEmb_Model_cutoff.to(device)\n",
    "    networkEmb_Model_cutoff = networkEmb_Model_cutoff.to(device)\n",
    "\n",
    "    splitEpochs = epochAmt\n",
    "    splitLR = 0.0001\n",
    "    textEmb_LR = 0.00001\n",
    "    networkEmb_LR = 0.00001\n",
    "\n",
    "    splitOptimizer = optim.Adam(splitModel.parameters(), lr=splitLR)\n",
    "    networkEmb_CutoffOptimizer = optim.Adam(networkEmb_Model_cutoff.parameters(), lr=networkEmb_LR)\n",
    "    textEmb_CutoffOptimizer = optim.Adam(textEmb_Model_cutoff.parameters(), lr=textEmb_LR)\n",
    "\n",
    "    bestModel = None\n",
    "    bestTestAcc = 0\n",
    "\n",
    "    for e in range(splitEpochs):\n",
    "        for idx, (userID, networkEmb, textEmb, blocked) in enumerate(trainData):\n",
    "\n",
    "            # Indicate that we're in training mode\n",
    "            networkEmb_Model_cutoff.train()\n",
    "            textEmb_Model_cutoff.train()\n",
    "            splitModel.train()\n",
    "\n",
    "            # Declaring a loss function\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "            # Perform a training step for the networkEmb_edding model \n",
    "            networkEmb_Scores = networkEmb_Model_cutoff(networkEmb)\n",
    "\n",
    "            # Perform a training step for the textEmb_ model\n",
    "            textEmb_Scores = textEmb_Model_cutoff(textEmb)\n",
    "\n",
    "            # Now, use these scores to perform a training step for the split model \n",
    "            inputScores = torch.cat((networkEmb_Scores, textEmb_Scores), dim=1)\n",
    "\n",
    "            splitScores = splitModel(inputScores)\n",
    "            splitScores = splitScores.reshape(blocked.shape)\n",
    "            splitLoss = loss_fn(splitScores, blocked)\n",
    "            splitOptimizer.zero_grad()\n",
    "            textEmb_CutoffOptimizer.zero_grad()\n",
    "            networkEmb_CutoffOptimizer.zero_grad()\n",
    "            splitLoss.backward()\n",
    "            splitOptimizer.step()\n",
    "            textEmb_CutoffOptimizer.step()\n",
    "            networkEmb_CutoffOptimizer.step()\n",
    "\n",
    "\n",
    "        # Print the accuracy of the split model\n",
    "        splitTestAcc = CheckAccuracy(testData, (networkEmb_Model_cutoff, textEmb_Model_cutoff, splitModel), \"split\", False)\n",
    "        splitTrainAcc = CheckAccuracy(trainData, (networkEmb_Model_cutoff, textEmb_Model_cutoff, splitModel), \"split\")\n",
    "\n",
    "        if (splitTestAcc > bestTestAcc):\n",
    "            bestModel = (copy.deepcopy(networkEmb_Model_cutoff), copy.deepcopy(textEmb_Model_cutoff), copy.deepcopy(splitModel))\n",
    "            bestTestAcc = splitTestAcc\n",
    "            \n",
    "    return (bestModel, bestTestAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72c69a",
   "metadata": {},
   "source": [
    "## Visualizations & Results\n",
    "\n",
    "Below, I used the best models that I'd been able to train, and then tested them on various different splits of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7d071ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'splitModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-3883665e277c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msplitTestAcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCheckAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'splitModel' is not defined"
     ]
    }
   ],
   "source": [
    "splitTestAcc = CheckAccuracy(loader_test, splitModel, \"split\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccda93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "networkTestAcc = CheckAccuracy(loader_test, networkEmb_model, \"network\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76872ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textTestAcc = CheckAccuracy(loader_test, textEmb_model, \"text\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2e8c59",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "After running the models a couple of times, I realized that their performance was varying a fair amount depending on what data was being used for training/testing. In order to ensure I had a solid idea of how each model was performing, I defined the following setup to perform cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bec2b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the folds, and a couple of data structures that we'll use\n",
    "# to store the results of the cross validation\n",
    "foldAmt = 4\n",
    "kfold = KFold(n_splits=foldAmt, shuffle=True)\n",
    "modelDict = {\"network\": [], \"text\": [], \"split\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "014e63e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CROSS VALIDATION - FOLD 1\n",
      "\n",
      "127\n",
      "43\n",
      "Training the social network model...\n",
      "Model recieved a test accuracy of 0.9302\n",
      "\n",
      "Training the text model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-3a867bb7e1fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Get the text model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training the text model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mtextModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestAcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetTextModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochAmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mmodelDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestAcc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model recieved a test accuracy of %.4f\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtestAcc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-587309d6b3b3>\u001b[0m in \u001b[0;36mgetTextModel\u001b[1;34m(trainData, testData, epochAmt)\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblocked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mtextEmb_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[0mtextEmb_opt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\trevb\\.conda\\envs\\personal\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\trevb\\.conda\\envs\\personal\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run through cross-validation for each of the different models\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(accounts)):\n",
    "    \n",
    "    # Print the fold you're on\n",
    "    print(\"\\n\\n\\nCROSS VALIDATION - FOLD %d\\n\" % (fold+1))\n",
    "    print(len(train_ids))\n",
    "    print(len(test_ids))\n",
    "    \n",
    "    # Declare the DataLoaders with this split of training / testing data\n",
    "    batchSize=4\n",
    "    trainSampler = SubsetRandomSampler(train_ids)\n",
    "    testSampler = SubsetRandomSampler(test_ids)\n",
    "    trainLoader = torch.utils.data.DataLoader(accounts, batch_size=batchSize, sampler=trainSampler)\n",
    "    testLoader = torch.utils.data.DataLoader(accounts, batch_size=batchSize, sampler=testSampler)\n",
    "    \n",
    "    # Get the social network model \n",
    "    print(\"Training the social network model...\")\n",
    "    networkModel, testAcc = getNetworkModel(trainLoader, testLoader, epochAmt=200)\n",
    "    modelDict[\"network\"].append((networkModel, testAcc))\n",
    "    print(\"Model recieved a test accuracy of %.4f\\n\" % testAcc)\n",
    "    \n",
    "    # Get the text model \n",
    "    print(\"Training the text model...\")\n",
    "    textModel, testAcc = getTextModel(trainLoader, testLoader, epochAmt=200)\n",
    "    modelDict[\"text\"].append((textModel, testAcc))\n",
    "    print(\"Model recieved a test accuracy of %.4f\\n\" % testAcc)\n",
    "    \n",
    "    # Get the split model \n",
    "    print(\"Training the split model...\")\n",
    "    splitModel, testAcc = getSplitModel(trainLoader, testLoader, epochAmt=300)\n",
    "    modelDict[\"split\"].append((splitModel, testAcc))\n",
    "    print(\"Model recieved a test accuracy of %.4f\\n\" % testAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc24a421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RUNNING FOLD 1\n",
      "Running CheckAccuracy() for the network model...\n",
      "Running CheckAccuracy() for the text model...\n",
      "Running CheckAccuracy() for the split model...\n",
      "Getting the accuracy of the bagging model...\n",
      "\n",
      "\n",
      "RUNNING FOLD 2\n",
      "Running CheckAccuracy() for the network model...\n",
      "Running CheckAccuracy() for the text model...\n",
      "Running CheckAccuracy() for the split model...\n",
      "Getting the accuracy of the bagging model...\n",
      "\n",
      "\n",
      "RUNNING FOLD 3\n",
      "Running CheckAccuracy() for the network model...\n",
      "Running CheckAccuracy() for the text model...\n",
      "Running CheckAccuracy() for the split model...\n",
      "Getting the accuracy of the bagging model...\n",
      "\n",
      "\n",
      "RUNNING FOLD 4\n",
      "Running CheckAccuracy() for the network model...\n",
      "Running CheckAccuracy() for the text model...\n",
      "Running CheckAccuracy() for the split model...\n",
      "Getting the accuracy of the bagging model...\n"
     ]
    }
   ],
   "source": [
    "# Run CheckAccuracy for each of the folds to get the average of each model accuracy\n",
    "runningAccDict = {\"network\": [], \"text\": [], \"split\": [], \"bagging\": []}\n",
    "runningF1Dict = {\"network\": [], \"text\": [], \"split\": [], \"bagging\": []}\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(accounts)): \n",
    "    \n",
    "    print(\"\\n\\nRUNNING FOLD %d\" % (fold+1))\n",
    "    \n",
    "    # Declare the DataLoaders with this split of training / testing data\n",
    "    batchSize=4\n",
    "    trainSampler = SubsetRandomSampler(train_ids)\n",
    "    testSampler = SubsetRandomSampler(test_ids)\n",
    "    trainLoader = torch.utils.data.DataLoader(accounts, batch_size=batchSize, sampler=trainSampler)\n",
    "    testLoader = torch.utils.data.DataLoader(accounts, batch_size=batchSize, sampler=testSampler)\n",
    "    \n",
    "    # Retrieving each of the models\n",
    "    networkModel = modelDict[\"network\"][fold][0]\n",
    "    textModel = modelDict[\"text\"][fold][0]\n",
    "    splitModel = modelDict[\"split\"][fold][0]\n",
    "    \n",
    "    # Getting the accuracies of each model \n",
    "    print(\"Running CheckAccuracy() for the network model...\")\n",
    "    testAcc_network, f1_network, confusionMatrix_network = CheckAccuracy(testLoader, networkModel, \"network\", True, True)\n",
    "    print(\"Running CheckAccuracy() for the text model...\")\n",
    "    testAcc_text, f1_text, confusionMatrix_text = CheckAccuracy(testLoader, textModel, \"text\", True, True)\n",
    "    print(\"Running CheckAccuracy() for the split model...\")\n",
    "    testAcc_split, f1_split, confusionMatrix_split = CheckAccuracy(testLoader, splitModel, \"split\", True, True)\n",
    "    \n",
    "    # Getting the accuracies of the \"bagged\" model\n",
    "    print(\"Getting the accuracy of the bagging model...\")\n",
    "    yTrue = []\n",
    "    yPred = []\n",
    "    networkModel.eval()\n",
    "    textModel.eval()\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for (userID, networkEmb, textEmb, blocked) in testLoader:\n",
    "            networkEmb_Scores = (torch.sigmoid(networkModel(networkEmb))).reshape(blocked.shape)\n",
    "            textEmb_Scores = (torch.sigmoid(textModel(textEmb))).reshape(blocked.shape)\n",
    "            bothScores = (networkEmb_Scores + textEmb_Scores)/2\n",
    "            binaryScores = torch.round(bothScores)\n",
    "            mask = (binaryScores == blocked)\n",
    "            yTrue = yTrue + list(blocked.cpu())\n",
    "            yPred = yPred + list(binaryScores.cpu())\n",
    "\n",
    "    # Grab some of the results from the bagging model \n",
    "    tn, fp, fn, tp = (confusion_matrix(yTrue, yPred).ravel())\n",
    "    testAcc_bagging = (tn + tp) / + (tn + fp + fn + tp)\n",
    "    f1_bagging = f1_score(yTrue, yPred)\n",
    "    confusionMatrix_bagging = (tn, fp, fn, tp)\n",
    "    \n",
    "    # Storing the results\n",
    "    runningAccDict[\"network\"].append(testAcc_network)\n",
    "    runningF1Dict[\"network\"].append(f1_network)\n",
    "    \n",
    "    runningAccDict[\"text\"].append(testAcc_text)\n",
    "    runningF1Dict[\"text\"].append(f1_text)\n",
    "    \n",
    "    runningAccDict[\"split\"].append(testAcc_split)\n",
    "    runningF1Dict[\"split\"].append(f1_split)\n",
    "    \n",
    "    runningAccDict[\"bagging\"].append(testAcc_bagging)\n",
    "    runningF1Dict[\"bagging\"].append(f1_bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86068089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Calculating averages for network...\n",
      "Average accuracy: 0.888\n",
      "Average F1 score: 0.887\n",
      "\n",
      "\n",
      "Calculating averages for text...\n",
      "Average accuracy: 0.948\n",
      "Average F1 score: 0.951\n",
      "\n",
      "\n",
      "Calculating averages for split...\n",
      "Average accuracy: 0.971\n",
      "Average F1 score: 0.968\n",
      "\n",
      "\n",
      "Calculating averages for bagging...\n",
      "Average accuracy: 0.953\n",
      "Average F1 score: 0.955\n"
     ]
    }
   ],
   "source": [
    "# Calculating averages for each model \n",
    "for modelType in [\"network\", \"text\", \"split\", \"bagging\"]:\n",
    "    print(\"\\n\\nCalculating averages for %s...\" % modelType)\n",
    "    print(\"Average accuracy: %.3f\" % (sum(runningAccDict[modelType]).item()/len(runningAccDict[modelType])))\n",
    "    print(\"Average F1 score: %.3f\" % (sum(runningF1Dict[modelType]).item()/len(runningF1Dict[modelType])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
